
# ========================================================================================
# Author	:		Koushik R
# Create date	:		2023-05-10
# Description	:		Python code to perform sentiment analysis on twitter data
# ========================================================================================					

#Required Packages

import logging
import azure.functions as func
from azure.storage.blob import BlobServiceClient, generate_blob_sas, BlobSasPermissions
from textblob import TextBlob
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import pandas as pd
import re
import string
from nltk.tokenize import RegexpTokenizer
import nltk
import os
from nltk.corpus import stopwords
nltk.download('omw-1.4')
nltk.download('wordnet')
nltk.download('stopwords')
import os
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
from textblob import TextBlob
STOPWORDS = set(stopwordlist)
english_punctuations = string.punctuation
punctuations_list = english_punctuations
tokenizer = RegexpTokenizer(r'\w+')
lemmatizer = WordNetLemmatizer()
from azure.keyvault.secrets import SecretClient
from azure.identity import DefaultAzureCredential

# ========================================================================================================================
#TextBlobAlgorithm

def textblob_algorithm(df):
    def getSubjectivity(text):
        return TextBlob(text).sentiment.subjectivity
            
            #Create a function to get the polarity
    def getPolarity(text):
        return TextBlob(text).sentiment.polarity
            
            #Create two new columns ‘Subjectivity’ & ‘Polarity’
    df['TextBlob_Subjectivity'] = df['cleaned_text'].apply(getSubjectivity)
    df['TextBlob_Polarity'] = df['cleaned_text'].apply(getPolarity)
    def getAnalysis(score):
        if score >-1 and score <-0.5:
            return 'Negative'
        elif score >= -0.5 and score <= 0.5:
            return 'Neutral'
        else:
            return 'Positive'
    df['TextBlob_Analysis'] = df['TextBlob_Polarity'].apply(getAnalysis )
    return df

# ========================================================================================================================
#VaderAlgorithm

def vader_algorithm(df):
    analyzer = SentimentIntensityAnalyzer()
    df['scores']=df['cleaned_text'].apply(lambda x: analyzer.polarity_scores(str(x)))
    df['compound']=df['scores'].apply(lambda score_dict:score_dict['compound'])
    df['pos']=df['scores'].apply(lambda pos_dict:pos_dict['pos'])
    df['neg']=df['scores'].apply(lambda neg_dict:neg_dict['neg'])
    def vader_analysis(compound):
        if compound > 0:
            return 'Positive'
        elif compound < 0 :
            return 'Negative'
        else:
            return 'Neutral'
    df['Emotion']= df['compound'].apply(lambda x : vader_analysis(x))
    return df

# ========================================================================================================================
#PreProcessingFunction

def getcleanedtext(text):
    text=text.lower()
    text = re.sub('@[\w]+','',text)
    
    def cleaning_stopwords(text):
        return " ".join([word for word in str(text).split() if word not in en_stopwords])
    stop_word_removal = cleaning_stopwords(text)
    
    def cleaning_punctuations(text):
        translator = str.maketrans('', '', punctuations_list)
        return text.translate(translator)
    punctuation_removal = cleaning_punctuations(stop_word_removal)
    
    def cleaning_repeating_char(text):
        return re.sub("(.)\\1{2,}", "\\1", text)
    repeating_char_removal = cleaning_repeating_char(punctuation_removal)
    
    def remove_urls(text):
        url_pattern = re.compile(r'https?://\S*|www\.\S+')
        return url_pattern.sub(r'', text)
    removed_url = remove_urls(repeating_char_removal)
    
    def remove_html(text):
        html_pattern = re.compile('<.*?>')
        return html_pattern.sub(r'', text)
    removed_html = remove_html(removed_url)
    
    def remove_emoji(text):
        emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags 
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
        return emoji_pattern.sub(r'', text)
    removed_emoji = remove_emoji(removed_html)
    
    def cleaning_numbers(data):
        return re.sub('[0-9]+', '', data)
    clean_numbers = cleaning_numbers(removed_emoji)
    
    def lemmatize_words(text):
        wordnet_map = {"N":wordnet.NOUN, "V":wordnet.VERB, "J":wordnet.ADJ, "R":wordnet.ADV}
        pos_tagged_text = nltk.pos_tag(text.split())
        return " ".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])
    lemmatize = lemmatize_words(clean_numbers)
    
    def remove_correction(text):
        return ' '.join(str(TextBlob(i).correct()) for i in text.split())
    spell_correction = remove_correction(lemmatize)
    
    def remove_white_space(data):
        return re.sub(' +', ' ',data) 
    removed_space = remove_white_space(spell_correction)
    removed_space = removed_space.strip()
    
    tokens=tokenizer.tokenize(removed_space)
    without_single_chr = [word for word in tokens if len(word) > 2]
    clean_text=' '.join(without_single_chr)
    return clean_text


# ========================================================================================================================
#EntryPointToTheProgram

def main(req: func.HttpRequest) -> func.HttpResponse:
    logging.info('Python HTTP trigger function processed a request.')
    file_name = ""
    account_name = req.params.get('account_name')
    container_name = req.params.get('container_name')
    blob_name = req.params.get('blob_name')
    column_name = req.params.get('column_name')
    algorithm_name = req.params.get('algorithm_name')
    try:
        if not account_name or container_name or blob_name or column_name or algorithm_name:
            try:
                req_body = req.get_json()
            except ValueError:
                pass
            else:
                account_name = req_body.get('account_name')
                container_name = req_body.get('container_name')
                blob_name = req_body.get('blob_name')
                column_name = req_body.get('column_name')
                algorithm_name = req_body.get('algorithm_name')
        if not blob_name==None:
            file_name = blob_name.split('/')[1]
            file_name_output = file_name.split('.')[0]
            
        #Accesing acceskey of storage account stored in Azure Key Vault using service principal auth.
        keyvaultname = os.getenv("keyvaultname")
        KVUri = f"https://{keyvaultname}.vault.azure.net"
        credential = DefaultAzureCredential()
        secretName = 'accesskey'
        client = SecretClient(vault_url=KVUri, credential=credential)
        retrieved_secret = client.get_secret(secretName)
        account_key = retrieved_secret.value
        
        #create a client to interact with blob storage
        connect_str = 'DefaultEndpointsProtocol=https;AccountName=' + account_name + ';AccountKey=' + account_key + ';EndpointSuffix=core.windows.net'
        blob_service_client = BlobServiceClient.from_connection_string(connect_str)

        #use the client to connect to the container
        container_client = blob_service_client.get_container_client(container_name)

        #generate a shared access signature for each blob file
        sas_i = generate_blob_sas(account_name = account_name,
                                            container_name = container_name,
                                            blob_name = blob_name,
                                            account_key=account_key,
                                            permission=BlobSasPermissions(read=True),
                                            expiry=datetime.utcnow() + timedelta(hours=1))
                
        sas_url = 'https://' + account_name+'.blob.core.windows.net/' + container_name + '/' + blob_name + '?' + sas_i

        #Read the blob using pandas library in python
        twitterdata= pd.read_csv(sas_url)
        X = twitterdata[column_name]

	    #Invoking Preprocessing function on the 'text' column
        x_clean=[getcleanedtext(i) for i in X]
        twitterdata['cleaned_text'] = x_clean

	    #Invoking the required algorithm based on the algorithm_name
        twitterdata = textblob_algorithm(twitterdata) if(algorithm_name=="textblob") else vader_algorithm(twitterdata)  

	    #Output the processed dataframe into a csv file
        analysed_output = twitterdata.to_csv (index_label="idx", encoding = "utf-8")

	    #Uploading the output into storage account 
        blob_client = blob_service_client.get_blob_client(container=container_name, blob="output/"+file_name_output+"_"+algorithm_name+"Output"+".csv") 
        blob_client.upload_blob(analysed_output,overwrite=True)
        return func.HttpResponse(
            analysed_output,
            status_code=200
        )
    except Exception as ex:
        return func.HttpResponse(
                "Some error at beginning itself"+logging.error(ex),
                status_code=200
            )
    finally:
        return func.HttpResponse(
                "Came to finally block atlast",
                status_code=200
            )
Requirement.txt
------------------     
azure.functions
TextBlob
pandas
vaderSentiment
azure.storage.blob
azure.keyvault.secrets
azure.identity
classification_report
scikit-learn

        
